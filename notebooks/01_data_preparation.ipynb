{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "`Overview`\n",
    "This notebook handles the initial data processing pipeline:\n",
    "- Loading raw data from source files\n",
    "- Performing exploratory data analysis (EDA)\n",
    "- Cleaning and handling missing values\n",
    "- Feature preprocessing and engineering\n",
    "- Exporting processed datasets for modeling\n",
    "\n",
    "`Inputs`\n",
    "- Raw data files from `../data/raw/` \n",
    "\n",
    "`Outputs`\n",
    "- Processed datasets in `../data/processed/`\n",
    "- EDA visualizations in `../reports/figures/`\n",
    "\n",
    "`Dependencies`\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "*Note: This is notebook 1 of the analysis pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Import data classes\n",
    "project_root = Path.cwd().parent  # assumes you're in /notebooks\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Import custom modules\n",
    "from backend.etl.ingestion import inspect_bad_lines, auto_fix_row, robust_csv_reader\n",
    "from backend.etl.cleaning import standardize_columns, clean_numeric_column, clean_date_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!which python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the project specific datasets as CSV files. In the follow-up cell, we load the auxiliary dataset containing extra information on the CORDIS-HORIZON projects. This includes\n",
    "- Scientific vocabulary \n",
    "- legal basis documents\n",
    "- organization\n",
    "- project\n",
    "- topics\n",
    "- webItem \n",
    "- webLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bertdepoorter/Nextcloud/EU_Horizon_Dashboard/notebooks\n"
     ]
    }
   ],
   "source": [
    "run_dir = os.getcwd()\n",
    "print(run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset as pandas DataFrame\n",
    "run_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(run_dir)\n",
    "\n",
    "raw_dir = f'{parent_dir}/data/raw'\n",
    "interim_dir = f'{parent_dir}/data/interim'\n",
    "processed_dir = f'{parent_dir}/data/processed'\n",
    "\n",
    "# define file paths to project-specific files\n",
    "data_report_path = f'{raw_dir}/reportSummaries.csv'\n",
    "data_filereport_path = f'{raw_dir}/file_report.csv'\n",
    "data_publications_path = f'{raw_dir}/projectPublications.csv'\n",
    "data_deliverables_path = f'{raw_dir}/projectDeliverables.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for cleaning\n",
    "The following functions are necessary to load the datasets correctly without manually changing them.\n",
    "- `inspect_bad_lines`: inspect lines that cannot be read directly\n",
    "- `auto_fix_row`: function that fixes row by merging excess columns together\n",
    "- `robust_csv_reader`: robust function that loads CSV files while applying `auto_fix_row` function on the bad lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage:\n",
    "```\n",
    "# check bad lines\n",
    "project_df, problematic_lines = inspect_bad_lines(project_path)\n",
    "\n",
    "# INspect how many bad lines there are \n",
    "print(f\"DataFrame loaded with {len(project_df)} rows.\")\n",
    "print(f\"Number of problematic lines: {len(problematic_lines)}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get DataFrame keys\n",
    "data_report = pd.read_csv(data_report_path, delimiter=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For key id:\n",
      "     0 elements are missing.\n",
      "For key title:\n",
      "     0 elements are missing.\n",
      "For key projectID:\n",
      "     0 elements are missing.\n",
      "For key projectAcronym:\n",
      "     0 elements are missing.\n",
      "For key attachment:\n",
      "     1861 elements are missing.\n",
      "For key contentUpdateDate:\n",
      "     0 elements are missing.\n",
      "For key rcn:\n",
      "     0 elements are missing.\n"
     ]
    }
   ],
   "source": [
    "# look for missing values\n",
    "report_missing = data_report.isnull()\n",
    "\n",
    "# check which columns are missing data\n",
    "for key in data_report:\n",
    "    missing = report_missing[report_missing[key] == True]\n",
    "    print(f'For key {key}:\\n     {len(missing.id)} elements are missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 problematic lines. Displaying the first 5:\n",
      "Line 1416: ['101081964_11_DELIVHORIZON', 'Guideline for trial implementation', 'Documents, reports', 'This guideline will summarise and present agricultural practices performed at BioMonitor4CAP farms and research sites with links to local/regional nature conservation goals and targeted species. Based on these baseline information the guideline will present how, when and to what extend research groups of WPs 2 and 4 are integrated into the activities of WP3. Supports specifically achieving following outcomes (Part B, section 2.3): A and B \"\"Strategy on monitoring soil biodiversity at farm scale adopted by science, end users, and policy\"\" and E \"\"Roadmap on expanding enhancing application and implementation of agri-environmental measures committing to the preservation of biodiversity', ' particularly agroforestry.\"\" This deliverable is an output of task 3.1.\"\"', '101081964', 'BioMonitor4CAP', 'https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5059d6c15&appId=PPGMS', 'Project deliverable', '2024-10-16 08:21:49', '1174701']\n",
      "Line 1424: ['101081964_23_DELIVHORIZON', 'BioMonitor4CAP webpage', 'Websites, patent fillings, videos etc.', 'Public project website including news, outputs, project info, contact info and relevant link. This deliverable is an output of task 6.2.Supports specifically achieving following outcome (Part B, section 2.3):  E \"\"Roadmap on expanding enhancing application and implementation of agri-environmental measures committing to the preservation of biodiversity', ' particularly agroforestry.\"\"\"\"', '101081964', 'BioMonitor4CAP', 'https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e504c0d848&appId=PPGMS', 'Project deliverable', '2024-10-16 08:21:44', '1174688']\n",
      "Line 6677: ['101061015_3_DELIVHORIZON', 'Initial Communication and Engagement Plan', 'Documents, reports', 'Deliverable 4.1 \"\"Initial Communication and Engagement Plan\"\" provides the strategy, communication and engagement guidelines with a set of tools and materials to ensure effective communication and a unique project visual identity. Delivered in M6. The deliverable will include the design and use guidelines of: i) website', ' ii) social media accounts on Facebook, Twitter, LinkedIn, Instagram and YouTube to cover scientific, policy, business and civil society domains and their stakeholder groups’ reach', ' iii) press releases, advertorials and web banners in various languages and hosted on news websites to promote the key messages and achievements of the project, raise public awareness about the related issues and increase engagement.It will receive input from Task T4.1. Linked to Task T4.2: Participatory approaches to communication, dissemination & stakeholder engagement.\"\"', '101061015', 'GeneBEcon', 'https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5f96fb30f&appId=PPGMS', 'Project deliverable', '2024-08-06 15:04:04', '1142320']\n",
      "Line 6678: ['101061015_9_DELIVHORIZON', 'Report on biosafety data requirements', 'Documents, reports', 'Deliverable 1.1 \"\"Report on biosafety data requirements\"\" is a compliance report based on the EU Directive 2001/18 and National Competent Authority requirements for biosafety that will report the risk assessment of 1) GM microalgae based on the QPS (Qualified Presumption of Safety) pipeline, leading to an EFSA risk evaluation', ' 2) PVY-resistant starch potato following available Competent Authority and EFSA Guidelines. PVY-resistant starch potato. Linked to Task T1.2: Data requirements for biosafety assessment of starch-enhanced, PVY-resistant potatoes and MAA-enhanced microalgae for biobased products under the five governance options\"\"', '101061015', 'GeneBEcon', 'https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e5095346f7&appId=PPGMS', 'Project deliverable', '2024-08-06 15:04:27', '1142273']\n",
      "Line 8812: ['101061189_7_DELIVHORIZON', 'Activities NitRecerCat 2023', 'Websites, patent fillings, videos etc.', \"Reporting activities of #NitRecerCat (list, participation, most successful activities possible lessons learnt…) delivered 30 days after the Night celebration. They will include:oA complete list of the researchers funded under FP7/Horizon 2020/Horizon Europe, with a specific focus on the Marie Skłodowska-Curie fellows, involved in the event with name, surname and project's grant agreement number\", 'oPromotional/audio-visual material such as: conferences, leaflets, posters, videos to be potentially used by the European Commission for promotional/communication purposes.oFurthermore, a disclaimer  will be visible at the entrance of the event in the national language of the event.\"\"', '101061189', 'NitRecerCat', 'https://ec.europa.eu/research/participants/documents/downloadPublic?documentIds=080166e50e94779f&appId=PPGMS', 'Project deliverable', '2024-07-08 15:44:27', '1127014']\n",
      "DataFrame loaded with 21913 rows.\n",
      "Number of problematic lines: 11\n"
     ]
    }
   ],
   "source": [
    "# Inspect Dataframe\n",
    "# account for problematic lines\n",
    "\n",
    "deliverables_df, problematic_lines = inspect_bad_lines(data_deliverables_path)\n",
    "\n",
    "print(f\"DataFrame loaded with {len(deliverables_df)} rows.\")\n",
    "print(f\"Number of problematic lines: {len(problematic_lines)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loaoding with the robust CSV rreader\n",
    "data_deliverables = robust_csv_reader(data_deliverables_path, expected_columns=10, problematic_column=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "Here we handle the missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are missing elements in the following columns:\n",
    "- deliverableType\n",
    "    - option 1: change to `'other'`\n",
    "    - option 2: look up individual titles and add manually\n",
    "- description\n",
    "    - option 1: add empty string\n",
    "    - Inspect manually to gain more insight what they exactly represent\n",
    "        - Update: all the titles related to the projects are quite related. I suggest we copy title values into the description column.\n",
    "- url\n",
    "    - 1 missing url. Add the url to the main page of this project (SELFY, id = 101069748_16_DELIVHORIZON) instead of link to deliverable?\n",
    "- rcn\n",
    "    - 1 rcn is missing. \n",
    "    - Looked this number up in publication list based on the projectAcronym = `'GeneBEcon'`. There the rcn number is gives as `1077637.0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Dataframe\n",
    "data_publications = pd.read_csv(data_publications_path, delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded with 24150 rows.\n",
      "Number of problematic lines: 0\n"
     ]
    }
   ],
   "source": [
    "publications_df, problematic_lines = inspect_bad_lines(data_publications_path, expected_columns=16)\n",
    "\n",
    "print(f\"DataFrame loaded with {len(publications_df)} rows.\")\n",
    "print(f\"Number of problematic lines: {len(problematic_lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "Here we inspect the missing data in this file, and outline how we are goiing to treat these missing data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For key authors:\n",
      "     75 elements are missing.\n",
      "For key journalTitle:\n",
      "     2099 elements are missing.\n",
      "For key journalNumber:\n",
      "     13622 elements are missing.\n",
      "For key publishedPages:\n",
      "     24142 elements are missing.\n",
      "For key issn:\n",
      "     7004 elements are missing.\n",
      "For key isbn:\n",
      "     23219 elements are missing.\n",
      "For key doi:\n",
      "     2293 elements are missing.\n"
     ]
    }
   ],
   "source": [
    "# look for missing values\n",
    "publications_missing = data_publications.isnull()\n",
    "\n",
    "# check which columns are missing data\n",
    "for key in publications_missing.keys():\n",
    "    missing = publications_missing[publications_missing[key] == True]\n",
    "    if len(missing.id) > 0:\n",
    "        print(f'For key {key}:\\n     {len(missing.id)} elements are missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite some missing data in this file. Let us go through each line individually.\n",
    "- authors:\n",
    "    - This sucks. Would have been very nice to decompose author strings into single authors and make the connections\n",
    "    - How to treat this: look into the article title string to check whether this one contains more author infromation\n",
    "- journalTitle:\n",
    "    - chack in the publication title. Sometimes there one has just copy-pasted the whole article reference\n",
    "- journalNumber:\n",
    "    - Not the most relevant parameter in my opinion. Just make all NaN zeros\n",
    "- publishedYear:\n",
    "    - Manually look this up\n",
    "- publishedPages:\n",
    "    - Not the most relevant parameter in my opinion. Just make all NaN zeros\n",
    "- issn:\n",
    "    - Not the most relevant parameter in my opinion. Just make all NaN zeros\n",
    "- isbn:\n",
    "    - Not the most relevant parameter in my opinion. Just make all NaN zeros\n",
    "- doi:\n",
    "    - Fuck this, just pass about:blank as url. \n",
    "- rcn:\n",
    "    - Manually adjust this one. \n",
    "        - Update: this entry was missing an entry for authors, all following field shifted 1 column to the left. Manually fixed this one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above does give an empty DataFrame. Used it to get the information to look up rcn number in other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to intermediate\n",
    "data_publications.to_csv(f'{interim_dir}/projectPublications_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect CORDIS-HORIZON projects data files\n",
    "This is the folder containing some more datasets on the different projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file paths\n",
    "SciVoc_path = f'{raw_dir}/euroSciVoc.csv'\n",
    "legalBasis_path = f'{raw_dir}/legalBasis.csv'\n",
    "organization_path = f'{raw_dir}/organization.csv'\n",
    "project_path = f'{raw_dir}/project.csv'\n",
    "topics_path = f'{raw_dir}/topics.csv'\n",
    "webItems_path = f'{raw_dir}/webItem.csv'\n",
    "webLink_path = f'{raw_dir}/webLink.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some informative files\n",
    "\n",
    "# load datasets\n",
    "read_csv_options = {\n",
    "    \"delimiter\": \";\",\n",
    "    \"quotechar\": '\"',\n",
    "    \"escapechar\": \"\\\\\",\n",
    "    'doublequote': False,\n",
    "    # \"on_bad_lines\": \"skip\",   # we skip lines that do not import properly for now\n",
    "    \"engine\": \"python\"  # 'python' engine handles complex parsing better\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciVoc dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "sci_voc_df = pd.read_csv(SciVoc_path, **read_csv_options)\n",
    "\n",
    "# clean\n",
    "from backend.etl.cleaning import clean_scivoc\n",
    "sci_voc_cleaned = clean_scivoc(sci_voc_df)\n",
    "\n",
    "# save to interim\n",
    "sci_voc_cleaned.to_csv(f'{interim_dir}/euroSciVoc_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## organization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "organization_df = pd.read_csv(organization_path, delimiter=';')\n",
    "\n",
    "# clean\n",
    "from backend.etl.cleaning import clean_organization\n",
    "organization_cleaned = clean_organization(organization_df)\n",
    "\n",
    "# save to interim\n",
    "organization_cleaned.to_csv(f'{interim_dir}/organization_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m topics_df = pd.read_csv(\u001b[43mtopics_path\u001b[49m, **read_csv_options)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# clean\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01metl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcleaning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clean_topics\n",
      "\u001b[31mNameError\u001b[39m: name 'topics_path' is not defined"
     ]
    }
   ],
   "source": [
    "# load\n",
    "topics_df = pd.read_csv(topics_path, **read_csv_options)\n",
    "\n",
    "# clean\n",
    "from backend.etl.cleaning import clean_topics\n",
    "topics_cleaned = clean_topics(topics_df)\n",
    "\n",
    "# save to interim\n",
    "topics_cleaned.to_csv(f'{interim_dir}/topics_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legal Basis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bertdepoorter/Nextcloud/EU_Horizon_Dashboard/backend/etl/cleaning.py:192: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['uniqueprogrammepart'] = df['uniqueprogrammepart'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "legal_basis_df = pd.read_csv(legalBasis_path, **read_csv_options)\n",
    "\n",
    "# clean\n",
    "from etl.cleaning import clean_legalbasis, standardize_columns\n",
    "legal_basis_cleaned = clean_legalbasis(legal_basis_df)\n",
    "\n",
    "# save to interim\n",
    "legal_basis_cleaned.to_csv(f'{interim_dir}/legalBasis_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## webItem / webLink dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "web_items_df = pd.read_csv(webItems_path, **read_csv_options)\n",
    "web_link_df = pd.read_csv(webLink_path, **read_csv_options)\n",
    "\n",
    "# clean\n",
    "\n",
    "from etl.cleaning import clean_webitem, clean_weblink\n",
    "web_items_cleaned = clean_webitem(web_items_df)\n",
    "web_link_cleaned = clean_weblink(web_link_df)\n",
    "\n",
    "# save to interim\n",
    "web_items_cleaned.to_csv(f'{interim_dir}/webItems_interim.csv', sep=';')\n",
    "web_link_cleaned.to_csv(f'{interim_dir}/webLink_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## projects dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 127 problematic lines. Displaying the first 5:\n",
      "Line 11: ['101114248', 'ELIA', 'CLOSED', 'Elia - Smart Assistant for English Learning', '2023-07-01', '2024-06-30', '0', '75000', 'HORIZON.3.2', 'HORIZON-EIE-2022-SCALEUP-02-02', '2023-06-05', 'HORIZON', 'HORIZON-EIE-2022-SCALEUP-02', 'HORIZON-EIE-2022-SCALEUP-02', 'HORIZON-CSA', '', \"We are a startup founded exclusively by women holding the top positions CEO and CTO. Our vision is to reduce inequalities by helping people own their English. That's why we created a personal assistant – Elia. Elia is a tool for busy professionals or swamped students struggling with their English. It connects English learning to their daily activities, e.g. writing an email at work\", ' watching videos on YouTube', ' or reading an article for a biology class. Because learning that is personalised and in context has been found to be the most effective form of learning. Elia started as a PhD project. Hence it\\'s based on insights from cognitive linguistics and developed using machine learning and natural language processing techniques. We presented it at conferences in Berkeley and Cambridge. The first demo version of Elia was launched at German universities in January 2022. Currently it has over 1600 users. We are about to close partnerships with two private language school and two student organisations: ISIC France and Spain. Since January, we have been working on improving the product to reach product-market fit. We reduced the churn rate (install to uninstall) from 19%  to 4% and increased the conversion rate (website to install) from 11% to 24%. Currently, 34% of users would be very disappointed if they could no longer use Elia. We are on our way to achieving product-market fit.We need the support of this grant, both financial and mentoring, to help us 1) reach the product-market-fit, 2) help us extend our partnerships and user base, 3) buy us time to find a venture capital investor. According to the Female Startup-Up Funding Index, \"\"In Austria, 0% of startups who raised money in the first half of 2022 have all-female founding teams and women make up 7% of the 174 founders who received funding.\"\" The numbers are no different elsewhere. Help us change these numbers. We are an all-female founding team. We could boost the \"\"0%\"\" number. But we need your help.\"\"', '2023-08-29 01:53:34', '253006', '10.3030/101114248']\n",
      "Line 24: ['101080692', 'FLUniversal', 'SIGNED', 'Intranasal, rapid-acting vaccine for all seasonal and pandemic influenza viruses', '2023-06-01', '2027-05-31', '7567787,5', '7567787,5', 'HORIZON.2.1', 'HORIZON-HLTH-2022-DISEASE-06-03-two-stage', '2023-05-08', 'HORIZON', 'HORIZON-HLTH-2022-DISEASE-06-two-stage', 'HORIZON-HLTH-2022-DISEASE-06-two-stage', 'HORIZON-RIA', '', 'Researchers worldwide have been working to develop a universal flu vaccine, but no breakthrough has yet been achieved. FLUniversal is not \"\"another costly universal flu project\"\". It is an opportunity to create a genuine universal flu vaccine that will set the standard for rapid, efficient vaccine development, and generate know-how and tools to develop next-generation vaccines. We plan to exploit our increased understanding of molecular mechanisms of influenza infection and immunity to develop a vaccine effective against all flu virus strains. Our innovation uses genetically modified flu strains administered intranasally in a prime-boost regimen. This approach rapidly induces interferon and broadly cross-neutralising antibodies in the nasal passages and a systemic immune response directed to the conserved HA stalk. Proof of concept for universal protection was demonstrated in the ferret', ' we propose now to show proof of concept in humans. Previous clinical studies established safety and immunogenicity in humans. The strains are efficiently produced in Vero cells. FLUniversal\\'s objectives align with the Expected Outcomes: comprehensive immunological assessment of preclinical models, development of a human challenge model, and assisting healthcare stakeholders’ decision-making about support for vaccine development. Consortium members’ world-leading expertise in preclinical models, clinical trials, immunology and validated assays will provide valuable insights on mechanisms of protection. Proposed clinical studies will provide crucial clinical proof of concept to advance the vaccine toward commercialisation.\"\"', '2023-06-28 16:54:09', '251023', '10.3030/101080692']\n",
      "Line 80: ['101113669', 'Pinova WTS Startup', 'CLOSED', 'Commercialization Pinova, a welfare technology start-up', '2023-06-01', '2024-02-29', '0', '75000', 'HORIZON.3.2', 'HORIZON-EIE-2022-SCALEUP-02-02', '2023-05-26', 'HORIZON', 'HORIZON-EIE-2022-SCALEUP-02', 'HORIZON-EIE-2022-SCALEUP-02', 'HORIZON-CSA', '', 'Our innovation addresses the societal challenge and market opportunity the ageing of the European population represents. Our innovation makes it possible to continue live safely at home for an extended period, despite age-related functional and mental impairment (UN Sustainable Development Goal #3).Using a non-intrusive system of sensors, algorithms, and artificial intelligence we identify incidents in elderly’s homes. With activity out-of-the-ordinary their loved ones automatically get alerted by a digital generated phone call, explaining the incident. Our innovation is user friendly', ' no alarm button, application or video surveillance is necessary. We want to penetrate the consumer marked with affordable welfare technology solutions. We are disrupting the industry with deep tech innovations and by using easily accessible sensor technology in a rapid growing market.  The current stage of our innovation is testing the beta-version in private homes. After the test period the greatest challenge for our startup and innovation is commercialization and market growth. By participating in The Woman Tech EU program, we aim at bringing our startup closer to commercialization, making proof of concept, product-marked-fit and go-to-market-strategy. The program will be a valued partner pursuing our goals. We believe that mentors from the program will be an important contribution in knowhow of business development. The opportunity to build an international network of experienced mentors and founders will both be inspirational and important. This program will also give us the opportunity to speed up activities.I’m an experienceI’m an experienced top-level manager, especially within organizational development, operations, and corporate finance, all at a national business level. Consequently, coaching and mentoring in marketing, growth strategy, and business development into international market would be of utmost value to me personally, and of great importance for Pinova.  \"\"', '2023-06-28 17:05:53', '251252', '10.3030/101113669']\n",
      "Line 97: ['101071747', 'InC', 'CLOSED', 'Innovative nanostructured aluminium composite', '2022-06-01', '2023-05-31', '0', '75000', 'HORIZON.3.2', 'HORIZON-EIE-2021-SCALEUP-01-03', '2022-05-25', 'HORIZON', 'HORIZON-EIE-2021-SCALEUP-01', 'HORIZON-EIE-2021-SCALEUP-01', 'HORIZON-CSA', '', \"Innovative nanostructured Aluminium composite. A constant goal in almost all areas of electronics is miniaturization. Miniaturization leads to a reduction in strip thicknesses and places ever higher demands on materials in terms of strength, conductivity, and formability. The trend towards transmitting higher currents goes hand in hand with higher demands on the base material. A key requirement for materials is thermal relaxation resistance, which enables it to maintain spring forces and transmit currents.Soluterials goal is to develop, produce and to offer nano-structured lightweight materials. Based on Ms. Kasakewitsch's scientific work and doctoral thesis, we developed a nanostructured aluminium material InC and transferred the production process from laboratory to pilot industrial scale. InC is the material that meets contemporary requirements and sets the course for the future.  It’s a key enabling technology that will lead to far-reaching developments in downstream industries. The material properties are needed in all future industries, such as e-mobility, 5G, and electronics. For example, electrical conductors such as high-voltage cables could be constructed as self-supporting materials in future\", ' in e-mobility, e-motors could be radically miniaturised.  InC opens up completely new possibilities for the miniaturisation of electrical components. In addition, like aluminium, InC can be produced cost-effectively on established production lines in one process step.Soluterials is a \"\"game-changer\"\" in a multipurpose metal design (our customers can streamline their production by using only one instead of several metals/alloys for their different products/applications). Soluterials is a knowledge provider, as we help our customers to develop new benefits and products with InC.\"\"', '2022-08-16 17:24:48', '239585', '10.3030/101071747']\n",
      "Line 220: ['101187327', 'OBICODE', 'SIGNED', 'Biocultural Transmissions of Obesity across Generations in the Digital Era', '2025-03-01', '2030-02-28', '0', '2499991,07', 'HORIZON.4.1', 'HORIZON-WIDERA-2023-TALENTS-01-01', '2024-10-29', 'HORIZON', 'HORIZON-WIDERA-2023-TALENTS-01', 'HORIZON-WIDERA-2023-TALENTS-01', 'HORIZON-CSA', '', 'OBICODE aims to significantly improve the research performance of the University of Coimbra (Portugal) by implementing a robust interdisciplinary research line in the field of Anthropology. It will do so under the leadership of one of the most respected and highly cited researchers in the field, Professor Stanley Ulijaszek, Emeritus Professor at the University of Oxford (#1 in Anthropology, QS ranking) and Director of the Biocultural Variation and Obesity Unit (UBVO).OBICODE will use the holism principle of Anthropology to develop and test an integration model of the social origins of obesity within and across generations, under the influence of the digital ecosystem, including web 2.0, and translate it into policy and intervention messages. Other objectives include: a) implement a Doctoral Programme on Integrative Models of Obesity', \" b) establish a Life course Obesity Observatory and c) reinforce citizens' scientific and health literacy under a new contemporary societal framework of Anthropology, encompassing an integrative and biocultural perspective of obesity.OBICODE will promote an institutional reform that aligns the University of Coimbra with cutting-edge anthropological research provided by Anthropology Departments/Centres in non-widening European countries. To achieve this goal, the institutional reform under OBICODE will focus on: 1) attracting talents in the area of Anthropology\", ' 2) improving research and education, by providing top notch training in an area that is central for resolving complex societal challenges such as obesity', ' and 3) bringing these scientific outputs to society. Overall, the scope of this action extends to the whole system: academia, target users (e.g., students), and the wider community, including policy and intervention. A team of prestigious and influential Anthropologists will advise on the different steps of the project, providing also a clear view for the future of Obesity studies.\"\"', '2024-11-08 14:34:31', '267247', '10.3030/101187327']\n",
      "DataFrame loaded with 15736 rows.\n",
      "Number of problematic lines: 127\n"
     ]
    }
   ],
   "source": [
    "project_df, problematic_lines = inspect_bad_lines(project_path, expected_columns=20)\n",
    "\n",
    "print(f\"DataFrame loaded with {len(project_df)} rows.\")\n",
    "print(f\"Number of problematic lines: {len(problematic_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_df = robust_csv_reader(project_path, expected_columns=20, problematic_column=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the dataset\n",
    "from backend.etl.cleaning import clean_project\n",
    "project_cleaned = clean_project(project_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to interim folder\n",
    "project_df.to_csv(f'{interim_dir}/project_interim.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct functions to access cleaned data\n",
    "\n",
    "We now define some functions that allow easy access to all aspects of different projects. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge datasets into one object\n",
    "- Standardize column names => they are compatible\n",
    "- Create function that allow access to project-specific data:\n",
    "    - function argument: project name / acronym / identifier\n",
    "    - function output: data class with project information as attributes\n",
    "    - Or: approach this from a class init perspective\n",
    "\n",
    "Find some way to pass load datasets\n",
    "apply class on this, without having to load the full dataset each time we initialize the class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Suleyman Ismayilov\\\\Desktop\\\\Suleyman\\\\EU_Horizon_Dashboard/data/interim/projectdeliverables_interim.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CORDIS_data\n\u001b[32m     10\u001b[39m parent_dir = project_root\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m Data_structure = \u001b[43mCORDIS_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Suleyman Ismayilov\\Desktop\\Suleyman\\EU_Horizon_Dashboard\\backend\\classes\\cordis_data.py:48\u001b[39m, in \u001b[36mCORDIS_data.__init__\u001b[39m\u001b[34m(self, parent_dir, enrich)\u001b[39m\n\u001b[32m     45\u001b[39m webLink_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.raw_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/webLink.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Load all datasets and set as class attributes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28mself\u001b[39m.data_report = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterim_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/projectdeliverables_interim.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.data_deliverables = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.interim_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/projectdeliverables_interim.csv\u001b[39m\u001b[33m'\u001b[39m, delimiter=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.data_publications = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.interim_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/projectPublications_interim.csv\u001b[39m\u001b[33m'\u001b[39m, delimiter=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Suleyman Ismayilov\\\\Desktop\\\\Suleyman\\\\EU_Horizon_Dashboard/data/interim/projectdeliverables_interim.csv'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the project root directory to sys.path so absolute imports work\n",
    "\n",
    "\n",
    "from backend.classes import CORDIS_data\n",
    "\n",
    "parent_dir = project_root\n",
    "Data_structure = CORDIS_data(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store feature-enriched dataframe to the processed directory\n",
    "Data_structure.export_dataframes(f'{processed_dir}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "0baab067-7ffb-46fe-8dc8-fa7754c21c7d",
       "rows": [
        [
         "0",
         "PvSeroRDT"
        ],
        [
         "1",
         "BIOBoost"
        ],
        [
         "2",
         "GlycanTrigger"
        ],
        [
         "3",
         "CHIKVAX_CHIM"
        ],
        [
         "4",
         "The Oater"
        ],
        [
         "5",
         "orthomobile"
        ],
        [
         "6",
         "EPiTB"
        ],
        [
         "7",
         "CECABI II"
        ],
        [
         "8",
         "GreenH2Wave"
        ],
        [
         "9",
         "ELIA"
        ],
        [
         "10",
         "CO-INVESTIN"
        ],
        [
         "11",
         "Avodes-Biotech"
        ],
        [
         "12",
         "COMBINE-CT"
        ],
        [
         "13",
         "GATBIO NEXT"
        ],
        [
         "14",
         "TWINRD"
        ],
        [
         "15",
         "DIO"
        ],
        [
         "16",
         "GEDEMAC"
        ],
        [
         "17",
         "NEXTNANO"
        ],
        [
         "18",
         "a-Doc"
        ],
        [
         "19",
         "CO2REMOVAL"
        ],
        [
         "20",
         "LIV"
        ],
        [
         "21",
         "CONNECTINGHEALTH"
        ],
        [
         "22",
         "FLUniversal"
        ],
        [
         "23",
         "Methanotrophy"
        ],
        [
         "24",
         "Cogo"
        ],
        [
         "25",
         "InnoAerogel"
        ],
        [
         "26",
         "DEEPSAFE"
        ],
        [
         "27",
         "Teleatherapy"
        ],
        [
         "28",
         "EARTHPULSE"
        ],
        [
         "29",
         "AdvoBots"
        ],
        [
         "30",
         "ModelMe"
        ],
        [
         "31",
         "Aiba"
        ],
        [
         "32",
         "SOLAR BABYSITTER"
        ],
        [
         "33",
         "SHARE"
        ],
        [
         "34",
         "Streem  Application"
        ],
        [
         "35",
         "Woman Tech EU"
        ],
        [
         "36",
         "Autonomous Dog Robot"
        ],
        [
         "37",
         "BirdShades WomenTech"
        ],
        [
         "38",
         "GloPID-R Sec III"
        ],
        [
         "39",
         "EndPD"
        ],
        [
         "40",
         "SMV delivery"
        ],
        [
         "41",
         "Envue"
        ],
        [
         "42",
         "NeuroRetina"
        ],
        [
         "43",
         "RAPID UTI"
        ],
        [
         "44",
         "Safehear"
        ],
        [
         "45",
         "TIME"
        ],
        [
         "46",
         "Drug Hunter"
        ],
        [
         "47",
         "GTMBL"
        ],
        [
         "48",
         "iComplai"
        ],
        [
         "49",
         "SusPhos"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 15053
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PvSeroRDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BIOBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GlycanTrigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHIKVAX_CHIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Oater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15048</th>\n",
       "      <td>EUCYS2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15049</th>\n",
       "      <td>RESAVER_2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15050</th>\n",
       "      <td>Leiden2022-ECS-ESOF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15051</th>\n",
       "      <td>EUCYS2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15052</th>\n",
       "      <td>CO-VALUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15053 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "0                PvSeroRDT\n",
       "1                 BIOBoost\n",
       "2            GlycanTrigger\n",
       "3             CHIKVAX_CHIM\n",
       "4                The Oater\n",
       "...                    ...\n",
       "15048            EUCYS2022\n",
       "15049         RESAVER_2023\n",
       "15050  Leiden2022-ECS-ESOF\n",
       "15051            EUCYS2024\n",
       "15052             CO-VALUE\n",
       "\n",
       "[15053 rows x 1 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_structure.list_of_acronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project_data(CORDIS_data):\n",
    "    def __init__(self, project_id=None, acronym=None):\n",
    "        # Inherit from CORDIS_data by initialzing parent class\n",
    "        super().__init__()\n",
    "\n",
    "        # Check if both project_id and acronym are provided\n",
    "        self.id, self.acronym = self._resolve_project_id_acronym(project_id, acronym)\n",
    "\n",
    "        # Add all project-specific data as attributes\n",
    "        \n",
    "        self.project_info = self._get_project_info()\n",
    "        self.publications = self._get_publications()\n",
    "        self.deliverables = self._get_deliverables()\n",
    "        self.organizations = self._get_organizations()\n",
    "        self.scivoc = self._get_scivoc()\n",
    "        self.topics = self._get_topics()\n",
    "        self.legal_basis = self._get_legal_basis()\n",
    "\n",
    "        # Add some enriched data\n",
    "        self.temporal_features = self._compute_temporal_features()\n",
    "        self.people_institutions = self._compute_people_institutions()\n",
    "        self.financial_metrics = self._compute_financial_metrics()\n",
    "        self.scientific_thematic = self._compute_scientific_thematic()\n",
    "\n",
    "    def _resolve_project_id_acronym(self, project_id, acronym):\n",
    "        if project_id is not None and acronym is not None:\n",
    "            expected_acronym = self.project_df[self.project_df['id'] == project_id]['acronym'].values[0]\n",
    "            if expected_acronym != acronym:\n",
    "                raise ValueError(f\"Acronym mismatch: ID {project_id} is linked to {expected_acronym}, not {acronym}.\")\n",
    "        elif acronym is not None:\n",
    "            project_id = self.project_df[self.project_df['acronym'] == acronym]['id'].values[0]\n",
    "        elif project_id is not None:\n",
    "            acronym = self.project_df[self.project_df['id'] == project_id]['acronym'].values[0]\n",
    "        else:\n",
    "            raise ValueError(\"Provide at least one of project_id or acronym.\")\n",
    "        return project_id, acronym\n",
    "\n",
    "    def _get_project_info(self):\n",
    "        return self.project_df[self.project_df['id'] == self.id].iloc[0].to_dict()\n",
    "\n",
    "    def _get_publications(self):\n",
    "        return self.data_publications[self.data_publications['projectID'] == self.id]\n",
    "\n",
    "    def _get_deliverables(self):\n",
    "        return self.data_deliverables[self.data_deliverables['projectID'] == self.id]\n",
    "\n",
    "    def _get_organizations(self):\n",
    "        return self.organization_df[self.organization_df['projectID'] == self.id]\n",
    "\n",
    "    def _get_scivoc(self):\n",
    "        return self.sci_voc_df[self.sci_voc_df['projectID'] == self.id]\n",
    "\n",
    "    def _get_topics(self):\n",
    "        return self.topics_df[self.topics_df['projectID'] == self.id]\n",
    "\n",
    "    def _get_legal_basis(self):\n",
    "        return self.legal_basis_df[self.legal_basis_df['projectID'] == self.id]\n",
    "    \n",
    "\n",
    "    # Add additional project features\n",
    "    def _compute_temporal_features(self):\n",
    "        fmt = \"%Y-%m-%d\"\n",
    "        start = self.project_info.get(\"startDate\", None)\n",
    "        end = self.project_info.get(\"endDate\", None)\n",
    "        ec_sig = self.project_info.get(\"ecSignatureDate\", None)\n",
    "\n",
    "        try:\n",
    "            start_date = datetime.strptime(start, fmt)\n",
    "            end_date = datetime.strptime(end, fmt)\n",
    "            duration_days = (end_date - start_date).days\n",
    "        except:\n",
    "            duration_days = None\n",
    "\n",
    "        return {\n",
    "            \"start_year\": start.year if start else None,\n",
    "            \"end_year\": end.year if end else None,\n",
    "            \"signature_year\": ec_sig[:4] if ec_sig else None,\n",
    "            \"duration_days\": duration_days\n",
    "        }\n",
    "    \n",
    "    def _compute_people_institutions(self):\n",
    "        orgs = self.organizations\n",
    "        if orgs.empty:\n",
    "            return {}\n",
    "        country_counts = orgs[\"country\"].value_counts().to_dict()\n",
    "        activity_types = orgs[\"activityType\"].value_counts().to_dict()\n",
    "        n_partners = orgs[\"organisationID\"].nunique()\n",
    "\n",
    "        return {\n",
    "            \"n_partners\": n_partners,\n",
    "            \"countries\": country_counts,\n",
    "            \"activity_types\": activity_types\n",
    "        }\n",
    "\n",
    "    def _compute_financial_metrics(self):\n",
    "        ec_total = self.project_info.get(\"ecMaxContribution\", None)\n",
    "        total_cost = self.project_info.get(\"totalCost\", None)\n",
    "        ec_partner_sum = self.organizations[\"ecContribution\"].sum()\n",
    "        cost_partner_sum = self.organizations[\"totalCost\"].sum()\n",
    "\n",
    "        try:\n",
    "            ec_per_deliverable = ec_total / len(self.deliverables)\n",
    "        except:\n",
    "            ec_per_deliverable = None\n",
    "\n",
    "        try:\n",
    "            ec_per_publication = ec_total / len(self.publications)\n",
    "        except:\n",
    "            ec_per_publication = None\n",
    "\n",
    "        return {\n",
    "            \"ec_total\": ec_total,\n",
    "            \"total_cost\": total_cost,\n",
    "            \"ec_sum_from_partners\": ec_partner_sum,\n",
    "            \"cost_sum_from_partners\": cost_partner_sum,\n",
    "            \"ec_per_deliverable\": ec_per_deliverable,\n",
    "            \"ec_per_publication\": ec_per_publication\n",
    "        }\n",
    "\n",
    "    def _compute_scientific_thematic(self):\n",
    "        scivoc_titles = self.scivoc['euroSciVocTitle'].dropna().unique().tolist()\n",
    "        topic_titles = self.topics['title'].dropna().unique().tolist()\n",
    "\n",
    "        pub_types = self.publications['isPublishedAs'].value_counts().to_dict()\n",
    "        deliverable_types = self.deliverables['deliverableType'].value_counts().to_dict()\n",
    "\n",
    "        return {\n",
    "            \"scivoc_keywords\": scivoc_titles,\n",
    "            \"topic_keywords\": topic_titles,\n",
    "            \"publication_types\": pub_types,\n",
    "            \"deliverable_types\": deliverable_types\n",
    "        }\n",
    "    def summary(self):\n",
    "        return {\n",
    "            \"project_id\": self.id,\n",
    "            \"acronym\": self.acronym,\n",
    "            \"title\": self.project_info.get(\"title\", \"\"),\n",
    "            \"temporal\": self.temporal_features,\n",
    "            \"institutions\": self.people_institutions,\n",
    "            \"financials\": self.financial_metrics,\n",
    "            \"keywords\": self.scientific_thematic\n",
    "        }\n",
    "    \n",
    "    def inspect_project_data(self):\n",
    "        \"\"\"\n",
    "        Print or return a structured overview of all enriched data for the selected project.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'id') or not hasattr(self, 'acronym'):\n",
    "            raise AttributeError(\"Please set a project using the `project()` method first.\")\n",
    "\n",
    "        print(f\"\\nProject: {self.acronym} (ID: {self.id})\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(\"\\nPublications:\")\n",
    "        pprint(getattr(self, 'publications', {}), indent=4)\n",
    "\n",
    "        print(\"\\nDeliverables:\")\n",
    "        pprint(getattr(self, 'deliverables', {})[['deliverableType', 'description']], indent=4)\n",
    "\n",
    "        print(\"\\nInstitutions / Organizations:\")\n",
    "        pprint(getattr(self, 'organizations', []), indent=4)\n",
    "\n",
    "        print(\"\\nFinancial Info:\")\n",
    "        pprint({\n",
    "            'Total Cost': getattr(self, 'total_cost', None),\n",
    "            'EC Contribution': getattr(self, 'ec_contribution', None),\n",
    "            'Num Orgs': getattr(self, 'num_organizations', None),\n",
    "            'Countries': getattr(self, 'countries', None),\n",
    "        }, indent=4)\n",
    "\n",
    "        print(\"\\nDates & Duration:\")\n",
    "        pprint({\n",
    "            'Start Date': getattr(self, 'start_date', None),\n",
    "            'End Date': getattr(self, 'end_date', None),\n",
    "            'Duration (days)': getattr(self, 'duration_days', None),\n",
    "            'Year': getattr(self, 'year', None)\n",
    "        }, indent=4)\n",
    "\n",
    "        print(\"\\nLegal & Administrative:\")\n",
    "        pprint({\n",
    "            'Legal Basis': getattr(self, 'legal_basis', None),\n",
    "            'Funding Scheme': getattr(self, 'funding_scheme', None),\n",
    "            'Framework Programme': getattr(self, 'framework_programme', None),\n",
    "        }, indent=4)\n",
    "\n",
    "        print(\"\\n🔬 Scientific Keywords (euroSciVoc):\")\n",
    "        pprint(getattr(self, 'sci_keywords', []), indent=4)\n",
    "\n",
    "        print(\"\\nProject Topics:\")\n",
    "        pprint(getattr(self, 'topics', []), indent=4)\n",
    "\n",
    "        print(\"\\nWeb Links:\")\n",
    "        pprint(getattr(self, 'web_links', []), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ec_total': 1622273.0, 'total_cost': 1622273.0, 'ec_sum_from_partners': np.float64(1622273.0), 'cost_sum_from_partners': '1622273', 'ec_per_deliverable': None, 'ec_per_publication': None}\n"
     ]
    }
   ],
   "source": [
    "p = Project_data(acronym=\"CLIMB\")\n",
    "summary = p.summary()\n",
    "print(summary[\"financials\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pprint to get out the background information in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cost_sum_from_partners': '1622273',\n",
      " 'ec_per_deliverable': None,\n",
      " 'ec_per_publication': None,\n",
      " 'ec_sum_from_partners': np.float64(1622273.0),\n",
      " 'ec_total': 1622273.0,\n",
      " 'total_cost': 1622273.0}\n"
     ]
    }
   ],
   "source": [
    "pprint(summary[\"financials\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project: BIOBoost (ID: 101096150)\n",
      "============================================================\n",
      "\n",
      "Publications:\n",
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, id, title, isPublishedAs, authors, journalTitle, journalNumber, publishedYear, publishedPages, issn, isbn, doi, projectID, projectAcronym, collection, contentUpdateDate, rcn]\n",
      "Index: []\n",
      "\n",
      "Deliverables:\n",
      "                              deliverableType  \\\n",
      "18191                      Documents, reports   \n",
      "18192                      Documents, reports   \n",
      "18193  Websites, patent fillings, videos etc.   \n",
      "18194                      Documents, reports   \n",
      "18195                    Data Management Plan   \n",
      "18196                      Documents, reports   \n",
      "\n",
      "                                             description  \n",
      "18191  The project management handbook will provide c...  \n",
      "18192  The PDEC provides information to all project p...  \n",
      "18193  Interactive online tool showing and mapping ma...  \n",
      "18194  The report includes information on innovation ...  \n",
      "18195  The DMP provides clear information on the cons...  \n",
      "18196  Assessment criteria for SMEs for potential amb...  \n",
      "\n",
      "Institutions / Organizations:\n",
      "    projectID projectAcronym  organisationID      vatNumber  \\\n",
      "8   101096150       BIOBoost       925710480     FI26896124   \n",
      "9   101096150       BIOBoost       984236206     DK25666070   \n",
      "10  101096150       BIOBoost       995548055  FR09489228908   \n",
      "11  101096150       BIOBoost       914459353            NaN   \n",
      "12  101096150       BIOBoost       937057540     SI51545535   \n",
      "13  101096150       BIOBoost       999456476            NaN   \n",
      "14  101096150       BIOBoost       914153512   PL8133709948   \n",
      "15  101096150       BIOBoost       999519720   PL7010073777   \n",
      "\n",
      "                                                 name  \\\n",
      "8                                  CLIC INNOVATION OY   \n",
      "9                                             FBCD AS   \n",
      "10                              BIOECONOMY FOR CHANGE   \n",
      "11  CLUSTER DE EMPRESAS DIGITALES SOTENIBLES E IND...   \n",
      "12   ITC - INOVACIJSKO TEHNOLOSKI GROZD MURSKA SOBOTA   \n",
      "13         VIESOJI ISTAIGA LIETUVOS INOVACIJU CENTRAS   \n",
      "14                                    FUNDACJA UNIMOS   \n",
      "15                   NARODOWE CENTRUM BADAN I ROZWOJU   \n",
      "\n",
      "                                            shortName    SME activityType  \\\n",
      "8                                  CLIC INNOVATION OY   True          OTH   \n",
      "9                                                FBCD   True          OTH   \n",
      "10                                                B4C   True          OTH   \n",
      "11                                  ONTECH INNOVATION  False          REC   \n",
      "12  ITC - INNOVATION TECHNOLOGY CLUSTER MURSKA SOBOTA  False          REC   \n",
      "13                                                LIC  False          PUB   \n",
      "14                                             UNIMOS  False          OTH   \n",
      "15                                               NCBR  False          PUB   \n",
      "\n",
      "                            street  postCode  ...  \\\n",
      "8                    ETELARANTA 10     00130  ...   \n",
      "9           NIELS PEDERSENS ALLE 2      8830  ...   \n",
      "10  10 RUE PIERRE GILLES DE GENNES     02000  ...   \n",
      "11          C/ MAESTRO MONTERO, 23     18004  ...   \n",
      "12              LENDAVSKA ULICA 5A      9000  ...   \n",
      "13                 T. Sevcenkos 13  LT-03223  ...   \n",
      "14               UL. GRZYBOWSKA 87    00 844  ...   \n",
      "15                 UL. CHMIELNA 69    00-801  ...   \n",
      "\n",
      "                                          contactForm    contentUpdateDate  \\\n",
      "8   https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "9   https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "10  https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "11  https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "12  https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "13  https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "14  https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "15  https://ec.europa.eu/info/funding-tenders/oppo...  2022-12-01 14:09:06   \n",
      "\n",
      "        rcn order         role ecContribution netEcContribution  totalCost  \\\n",
      "8   2573619     8  participant        68562.5           68562.5          0   \n",
      "9   2158736     1  coordinator        91812.5           91812.5          0   \n",
      "10  2136166     6  participant        65937.5           65937.5          0   \n",
      "11  1972562     5  participant        42562.5           42562.5          0   \n",
      "12  1958893     4  participant        60625.0           60625.0          0   \n",
      "13  2111621     3  participant        56312.5           56312.5          0   \n",
      "14  1966169     2  participant        73000.0           73000.0          0   \n",
      "15  1906778     7  participant        41187.5           41187.5          0   \n",
      "\n",
      "    endOfParticipation active  \n",
      "8                False    NaN  \n",
      "9                False    NaN  \n",
      "10               False    NaN  \n",
      "11               False    NaN  \n",
      "12               False    NaN  \n",
      "13               False    NaN  \n",
      "14               False    NaN  \n",
      "15               False    NaN  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "\n",
      "Financial Info:\n",
      "{   'Countries': None,\n",
      "    'EC Contribution': None,\n",
      "    'Num Orgs': None,\n",
      "    'Total Cost': None}\n",
      "\n",
      "Dates & Duration:\n",
      "{'Duration (days)': None, 'End Date': None, 'Start Date': None, 'Year': None}\n",
      "\n",
      "Legal & Administrative:\n",
      "{   'Framework Programme': None,\n",
      "    'Funding Scheme': None,\n",
      "    'Legal Basis':    projectID     legalBasis                                 title  \\\n",
      "1  101096150    HORIZON.3.2        European innovation ecosystems   \n",
      "2  101096150  HORIZON.3.2.3  Joint programmes close to innovators   \n",
      "\n",
      "  uniqueProgrammePart  \n",
      "1                True  \n",
      "2                 NaN  }\n",
      "\n",
      "🔬 Scientific Keywords (euroSciVoc):\n",
      "[]\n",
      "\n",
      "Project Topics:\n",
      "   projectID                           topic  \\\n",
      "1  101096150  HORIZON-EIE-2022-CONNECT-01-01   \n",
      "\n",
      "                                               title  \n",
      "1  Towards more inclusive networks and initiative...  \n",
      "\n",
      "Web Links:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Inspect a certain project\n",
    "p = Project_data(acronym=\"BIOBoost\")\n",
    "p.inspect_project_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
